{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a99944e-dfa9-4def-aa5b-726aa6d56621",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99476ff4a2014ec0aa59ee9ca4aa1424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"gt_parse\": {\"header\": {\"invoice_no\": \"081553517\", \"invoice_date\": \"2/2/2023\", \"salesOrderNumber\": \"71555\", \"poNumber\": \"21129-0101.1\"}, \"items\": [{\"unitPrice\": \"0.63\", \"description\": \"ARMORED CABLE BX 12-2 COILS\", \"quantity\": \"10,000\"}, {\"unitPrice\": \"1.74\", \"description\": \"ARMORED CABLE MC 10-2\", \"quantity\": \"500\"}, {\"unitPrice\": \"2.66488\", \"quantity\": \"10\", \"description\": \"ALUM.LUG DOUI JBLE 1/0 10/100-PK\"}, {\"unitPrice\": \"0.29\", \"description\": \"10STRANDED BLACK\", \"quantity\": \"1,000\"}, {\"unitPrice\": \"0.29\", \"description\": \"10STRANDED GREEN\", \"quantity\": \"500\"}, {\"unitPrice\": \"0.29\", \"description\": \"10STRANDED RED\", \"quantity\": \"1,000\"}, {\"unitPrice\": \"0.29\", \"description\": \"10STRANDED WHITE\", \"quantity\": \"1,000\"}, {\"unitPrice\": \"0.94063\", \"description\": \"2\\\" KNOCKOUT PLUG 10/100PK\", \"quantity\": \"6\"}, {\"unitPrice\": \"0.125\", \"quantity\": \"10\", \"description\": \"1\\\"X3/4\\\" RED WASH 100/1000-PK\"}, {\"unitPrice\": \"0.28288\", \"quantity\": \"10\", \"description\": \"1-1/2X3/4REDWASH50/500-PK\"}, {\"unitPrice\": \"0.28288\", \"quantity\": \"10\", \"description\": \"1-1/2\\\"X1\\\"RED WASH 50/500-PK\"}, {\"unitPrice\": \"2.00\", \"description\": \"ARMORED CABLE MC 10-3 COILS\", \"quantity\": \"500\"}, {\"unitPrice\": \"0.64\", \"description\": \"ARMORED CABLE MC 12-2 COIL\", \"quantity\": \"250\"}, {\"unitPrice\": \"29.18045\", \"quantity\": \"3\", \"description\": \"3\\\"x1000' 2MIL. YELLOW CAUTION TAPE\"}]}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(r\"D:\\sparrow-main\\sparrow-data\\docs\\models\\donut\\data\")\n",
    "dataset\n",
    "example = dataset['train'][0]\n",
    "image = example['image']\n",
    "# let's make the image a bit smaller when visualizing\n",
    "width, height = image.size\n",
    "#display(image.resize((int(width*0.3), int(height*0.3))))\n",
    "# let's load the corresponding JSON dictionary (as string representation)\n",
    "ground_truth = example['ground_truth']\n",
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b2e9b1f-ea9e-485f-a963-608b1f95ffaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "literal_eval(ground_truth)['gt_parse']\n",
    "from transformers import VisionEncoderDecoderConfig\n",
    "\n",
    "image_size = [1275, 1650]\n",
    "max_length = 768\n",
    "\n",
    "# update image_size of the encoder\n",
    "# during pre-training, a larger image size was used\n",
    "config = VisionEncoderDecoderConfig.from_pretrained(r\"C:\\Users\\Deepesh Alwani\\Desktop\\invoices-donut-model-v1\")\n",
    "config.encoder.image_size = image_size # (height, width)\n",
    "# update max_length of the decoder (for generation)\n",
    "config.decoder.max_length = max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb61a1f5-6fbe-4f4f-9e1a-413b65480c33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO we should actually update max_position_embeddings and interpolate the pre-trained ones:\n",
    "# https://github.com/clovaai/donut/blob/0acc65a85d140852b8d9928565f0f6b2d98dc088/donut/model.py#L602\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(r\"C:\\Users\\Deepesh Alwani\\Desktop\\invoices-donut-model-v1\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(r\"C:\\Users\\Deepesh Alwani\\Desktop\\invoices-donut-model-v1\", config=config)\n",
    "import json\n",
    "import random\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "added_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eca24ab1-ed61-410b-9638-4fe9c42f9629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DonutDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for Donut. This class takes a HuggingFace Dataset as input.\n",
    "    \n",
    "    Each row, consists of image path(png/jpg/jpeg) and gt data (json/jsonl/txt),\n",
    "    and it will be converted into pixel_values (vectorized image) and labels (input_ids of the tokenized string).\n",
    "    \n",
    "    Args:\n",
    "        dataset_name_or_path: name of dataset (available at huggingface.co/datasets) or the path containing image files and metadata.jsonl\n",
    "        max_length: the max number of tokens for the target sequences\n",
    "        split: whether to load \"train\", \"validation\" or \"test\" split\n",
    "        ignore_id: ignore_index for torch.nn.CrossEntropyLoss\n",
    "        task_start_token: the special token to be fed to the decoder to conduct the target task\n",
    "        prompt_end_token: the special token at the end of the sequences\n",
    "        sort_json_key: whether or not to sort the JSON keys\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name_or_path: str,\n",
    "        max_length: int,\n",
    "        split: str = \"train\",\n",
    "        ignore_id: int = -100,\n",
    "        task_start_token: str = \"<s>\",\n",
    "        prompt_end_token: str = None,\n",
    "        sort_json_key: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.ignore_id = ignore_id\n",
    "        self.task_start_token = task_start_token\n",
    "        self.prompt_end_token = prompt_end_token if prompt_end_token else task_start_token\n",
    "        self.sort_json_key = sort_json_key\n",
    "\n",
    "        self.dataset = load_dataset(dataset_name_or_path, split=self.split)\n",
    "        self.dataset_length = len(self.dataset)\n",
    "\n",
    "        self.gt_token_sequences = []\n",
    "        for sample in self.dataset:\n",
    "            ground_truth = json.loads(sample[\"ground_truth\"])\n",
    "            if \"gt_parses\" in ground_truth:  # when multiple ground truths are available, e.g., docvqa\n",
    "                assert isinstance(ground_truth[\"gt_parses\"], list)\n",
    "                gt_jsons = ground_truth[\"gt_parses\"]\n",
    "            else:\n",
    "                assert \"gt_parse\" in ground_truth and isinstance(ground_truth[\"gt_parse\"], dict)\n",
    "                gt_jsons = [ground_truth[\"gt_parse\"]]\n",
    "\n",
    "            self.gt_token_sequences.append(\n",
    "                [\n",
    "                    self.json2token(\n",
    "                        gt_json,\n",
    "                        update_special_tokens_for_json_key=self.split == \"train\",\n",
    "                        sort_json_key=self.sort_json_key,\n",
    "                    )\n",
    "                    + processor.tokenizer.eos_token\n",
    "                    for gt_json in gt_jsons  # load json from list of json\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.add_tokens([self.task_start_token, self.prompt_end_token])\n",
    "        self.prompt_end_token_id = processor.tokenizer.convert_tokens_to_ids(self.prompt_end_token)\n",
    "\n",
    "    def json2token(self, obj: Any, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n",
    "        \"\"\"\n",
    "        Convert an ordered JSON object into a token sequence\n",
    "        \"\"\"\n",
    "        if type(obj) == dict:\n",
    "            if len(obj) == 1 and \"text_sequence\" in obj:\n",
    "                return obj[\"text_sequence\"]\n",
    "            else:\n",
    "                output = \"\"\n",
    "                if sort_json_key:\n",
    "                    keys = sorted(obj.keys(), reverse=True)\n",
    "                else:\n",
    "                    keys = obj.keys()\n",
    "                for k in keys:\n",
    "                    if update_special_tokens_for_json_key:\n",
    "                        self.add_tokens([fr\"<s_{k}>\", fr\"</s_{k}>\"])\n",
    "                    output += (\n",
    "                        fr\"<s_{k}>\"\n",
    "                        + self.json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
    "                        + fr\"</s_{k}>\"\n",
    "                    )\n",
    "                return output\n",
    "        elif type(obj) == list:\n",
    "            return r\"<sep/>\".join(\n",
    "                [self.json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
    "            )\n",
    "        else:\n",
    "            obj = str(obj)\n",
    "            if f\"<{obj}/>\" in added_tokens:\n",
    "                obj = f\"<{obj}/>\"  # for categorical special tokens\n",
    "            return obj\n",
    "    \n",
    "    def add_tokens(self, list_of_tokens: List[str]):\n",
    "        \"\"\"\n",
    "        Add special tokens to tokenizer and resize the token embeddings of the decoder\n",
    "        \"\"\"\n",
    "        newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n",
    "        if newly_added_num > 0:\n",
    "            model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "            added_tokens.extend(list_of_tokens)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Load image from image_path of given dataset_path and convert into input_tensor and labels\n",
    "        Convert gt data into input_ids (tokenized string)\n",
    "        Returns:\n",
    "            input_tensor : preprocessed image\n",
    "            input_ids : tokenized gt_data\n",
    "            labels : masked labels (model doesn't need to predict prompt and pad token)\n",
    "        \"\"\"\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        # inputs\n",
    "        pixel_values = processor(sample[\"image\"], random_padding=self.split == \"train\", return_tensors=\"pt\").pixel_values\n",
    "        pixel_values = pixel_values.squeeze()\n",
    "\n",
    "        # targets\n",
    "        target_sequence = random.choice(self.gt_token_sequences[idx])  # can be more than one, e.g., DocVQA Task 1\n",
    "        input_ids = processor.tokenizer(\n",
    "            target_sequence,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = self.ignore_id  # model doesn't need to predict pad token\n",
    "        # labels[: torch.nonzero(labels == self.prompt_end_token_id).sum() + 1] = self.ignore_id  # model doesn't need to predict prompt (for VQA)\n",
    "        return pixel_values, labels, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b4537fc-e88d-406d-8ca2-521f5ed68e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ccd16163cb44d3ac7159c44a566fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6ee8cf2cfa4eecb5ec61d70146769d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s_salesOrderNumber>', '</s_salesOrderNumber>', '<s_poNumber>', '</s_poNumber>', '<s_unitPrice>', '</s_unitPrice>', '<s_description>', '</s_description>', '<s_quantity>', '</s_quantity>', '<s_S.O.NO>', '</s_S.O.NO>', '<s_P.O. NO>', '</s_P.O. NO>', '<s_price_one_item>', '</s_price_one_item>']\n",
      "Original number of tokens: 57522\n",
      "Number of tokens after adding special tokens: 57582\n",
      "torch.Size([3, 1275, 1650])\n",
      "<s_header>\n",
      "<s_invoice_no>\n",
      "08\n",
      "15\n",
      "5\n",
      "35\n",
      "17\n",
      "</s_invoice_no>\n",
      "<s_invoice_date>\n",
      "2\n",
      "/2\n",
      "/20\n",
      "23\n",
      "</s_invoice_date>\n",
      "<s_salesOrderNumber>\n",
      "71\n",
      "555\n",
      "</s_salesOrderNumber>\n",
      "<s_poNumber>\n",
      "2\n",
      "11\n",
      "29\n",
      "-\n",
      "010\n",
      "1.1\n",
      "</s_poNumber>\n",
      "</s_header>\n",
      "<s_items>\n",
      "<s_unitPrice>\n",
      "0\n",
      "<s_header><s_invoice_no>081553517</s_invoice_no><s_invoice_date>2/2/2023</s_invoice_date><s_salesOrderNumber>71555</s_salesOrderNumber><s_poNumber>21129-0101.1</s_poNumber></s_header><s_items><s_unitPrice>0.63</s_unitPrice><s_description>ARMORED CABLE BX 12-2 COILS</s_description><s_quantity>10,000</s_quantity><sep/><s_unitPrice>1.74</s_unitPrice><s_description>ARMORED CABLE MC 10-2</s_description><s_quantity>500</s_quantity><sep/><s_unitPrice>2.66488</s_unitPrice><s_quantity>10</s_quantity><s_description>ALUM.LUG DOUI JBLE 1/0 10/100-PK</s_description><sep/><s_unitPrice>0.29</s_unitPrice><s_description>10STRANDED BLACK</s_description><s_quantity>1,000</s_quantity><sep/><s_unitPrice>0.29</s_unitPrice><s_description>10STRANDED GREEN</s_description><s_quantity>500</s_quantity><sep/><s_unitPrice>0.29</s_unitPrice><s_description>10STRANDED RED</s_description><s_quantity>1,000</s_quantity><sep/><s_unitPrice>0.29</s_unitPrice><s_description>10STRANDED WHITE</s_description><s_quantity>1,000</s_quantity><sep/><s_unitPrice>0.94063</s_unitPrice><s_description>2\" KNOCKOUT PLUG 10/100PK</s_description><s_quantity>6</s_quantity><sep/><s_unitPrice>0.125</s_unitPrice><s_quantity>10</s_quantity><s_description>1\"X3/4\" RED WASH 100/1000-PK</s_description><sep/><s_unitPrice>0.28288</s_unitPrice><s_quantity>10</s_quantity><s_description>1-1/2X3/4REDWASH50/500-PK</s_description><sep/><s_unitPrice>0.28288</s_unitPrice><s_quantity>10</s_quantity><s_description>1-1/2\"X1\"RED WASH 50/500-PK</s_description><sep/><s_unitPrice>2.00</s_unitPrice><s_description>ARMORED CABLE MC 10-3 COILS</s_description><s_quantity>500</s_quantity><sep/><s_unitPrice>0.64</s_unitPrice><s_description>ARMORED CABLE MC 12-2 COIL</s_description><s_quantity>250</s_quantity><sep/><s_unitPrice>29.18045</s_unitPrice><s_quantity>3</s_quantity><s_description>3\"x1000' 2MIL. YELLOW CAUTION TAPE</s_description></s_items></s>\n",
      "Pad token ID: <pad>\n",
      "Decoder start token ID: <s_cord-v2>\n"
     ]
    }
   ],
   "source": [
    "# we update some settings which differ from pretraining; namely the size of the images + no rotation required\n",
    "# source: https://github.com/clovaai/donut/blob/master/config/train_cord.yaml\n",
    "processor.image_processor.size = image_size[::-1] # should be (width, height)\n",
    "processor.image_processor.do_align_long_axis = False\n",
    "\n",
    "train_dataset = DonutDataset(r\"D:\\sparrow-main\\sparrow-data\\docs\\models\\donut\\data\\img\", max_length=max_length,\n",
    "                             split=\"train\", task_start_token=\"<s_cord-v2>\", prompt_end_token=\"<s_cord-v2>\",\n",
    "                             sort_json_key=False, # cord dataset is preprocessed, so no need for this\n",
    "                             )\n",
    "\n",
    "val_dataset = DonutDataset(r\"D:\\sparrow-main\\sparrow-data\\docs\\models\\donut\\data\\img\", max_length=max_length,\n",
    "                             split=\"validation\", task_start_token=\"<s_cord-v2>\", prompt_end_token=\"<s_cord-v2>\",\n",
    "                             sort_json_key=False, # cord dataset is preprocessed, so no need for this\n",
    "                             )\n",
    "len(added_tokens)\n",
    "print(added_tokens)\n",
    "# the vocab size attribute stays constants (might be a bit unintuitive - but doesn't include special tokens)\n",
    "print(\"Original number of tokens:\", processor.tokenizer.vocab_size)\n",
    "print(\"Number of tokens after adding special tokens:\", len(processor.tokenizer))\n",
    "processor.decode([57521])\n",
    "pixel_values, labels, target_sequence = train_dataset[0]\n",
    "print(pixel_values.shape)\n",
    "# let's print the labels (the first 30 token ID's)\n",
    "for id in labels.tolist()[:30]:\n",
    "  if id != -100:\n",
    "    print(processor.decode([id]))\n",
    "  else:\n",
    "    print(id)\n",
    "# let's check the corresponding target sequence, as a string\n",
    "print(target_sequence)\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(['<s_cord-v2>'])[0]\n",
    "# sanity check\n",
    "print(\"Pad token ID:\", processor.decode([model.config.pad_token_id]))\n",
    "print(\"Decoder start token ID:\", processor.decode([model.config.decoder_start_token_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14914c2f-cbab-47fe-aee8-8e293b794fb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x0000028D3DF0E090>\n",
      "torch.Size([1, 3, 1275, 1650])\n",
      "<s_header>\n",
      "<s_invoice_no>\n",
      "08\n",
      "15\n",
      "59\n",
      "25\n",
      "2\n",
      "</s_invoice_no>\n",
      "<s_invoice_date>\n",
      "10\n",
      "/18\n",
      "/20\n",
      "23\n",
      "</s_invoice_date>\n",
      "<s_S.O.NO>\n",
      "77\n",
      "30\n",
      "7\n",
      "</s_S.O.NO>\n",
      "<s_P.O. NO>\n",
      "23\n",
      "133\n",
      "-01\n",
      "65\n",
      ".\n",
      "3\n",
      "</s_P.O. NO>\n",
      "</s_header>\n",
      "<s_items>\n",
      "<s_item_qty>\n",
      "23\n",
      "5\n",
      "torch.Size([1, 3, 1275, 1650])\n",
      "<s_header><s_invoice_no>081558399</s_invoice_no><s_invoice_date>9/11/2023</s_invoice_date><s_salesOrderNumber>76352</s_salesOrderNumber><s_poNumber>22131-0310.1</s_poNumber></s_header><s_items><s_quantity>100</s_quantity><s_description>4SQBX2-1/8DP 1/2-3/4TKO BMP</s_description><sep/><s_quantity>20</s_quantity><s_description>7/8\"HOLESAW</s_description><sep/><s_quantity>20</s_quantity><s_description>6SOL-16SOL SPLITBOLT 100/1000PK</s_description><sep/><s_quantity>4</s_quantity><s_description>3\"LOCKNUTSTEEL50-PK</s_description><sep/><s_quantity>2</s_quantity><s_description>3\"PLASTIC BUSHING25-PK</s_description><sep/><s_quantity>2</s_quantity><s_description>3\"EMT GF CHANGE OVER CONN</s_description><sep/><s_quantity>6</s_quantity><s_description>500 2PORT</s_description><sep/><s_quantity>15</s_quantity><s_description>3501 MCM BLACK</s_description><sep/><s_quantity>300</s_quantity><s_description>3/8\" SADDLE CONN 50/500PK</s_description><sep/><s_quantity>200</s_quantity><s_description>3/8\"DUPLEX CONN 25/250-PK</s_description><sep/><s_quantity>10</s_quantity><s_description>A-SHORT #035-PCS PER BAG-500</s_description><sep/><s_quantity>100</s_quantity><s_description>3/4\"STRUT CLAMP 100/PK.</s_description><sep/><s_quantity>3</s_quantity><s_description>6x6x4 NEMA 1 SC BOX</s_description><sep/><s_quantity>1,000</s_quantity><s_description>B-CAP BAGI RED WIRENUTS</s_description><sep/><s_description>3\"x4\" CONDUIT NIPPLE 1/15-PK</s_description><s_quantity>1</s_quantity><sep/><s_description>3\"EMT COMPCONN ISTEEL 12PK</s_description><s_quantity>1</s_quantity></s_items></s>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# feel free to increase the batch size if you have a lot of memory\n",
    "# I'm fine-tuning on Colab and given the large image size, batch size > 1 is not feasible\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "print(train_dataloader)\n",
    "batch = next(iter(train_dataloader))\n",
    "pixel_values, labels, target_sequences = batch\n",
    "print(pixel_values.shape)\n",
    "for id in labels.squeeze().tolist()[:30]:\n",
    "  if id != -100:\n",
    "    print(processor.decode([id]))\n",
    "  else:\n",
    "    print(id)\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "# let's check the first validation batch\n",
    "batch = next(iter(val_dataloader))\n",
    "pixel_values, labels, target_sequences = batch\n",
    "print(pixel_values.shape)\n",
    "print(target_sequences[0])\n",
    "from pathlib import Path\n",
    "import re\n",
    "from nltk import edit_distance\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "\n",
    "\n",
    "class DonutModelPLModule(pl.LightningModule):\n",
    "    def __init__(self, config, processor, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pixel_values, labels, _ = batch\n",
    "        \n",
    "        outputs = self.model(pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "        pixel_values, labels, answers = batch\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        # we feed the prompt to the model\n",
    "        decoder_input_ids = torch.full((batch_size, 1), self.model.config.decoder_start_token_id, device=self.device)\n",
    "        \n",
    "        outputs = self.model.generate(pixel_values,\n",
    "                                   decoder_input_ids=decoder_input_ids,\n",
    "                                   max_length=max_length,\n",
    "                                   early_stopping=True,\n",
    "                                   pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                                   eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "                                   use_cache=True,\n",
    "                                   num_beams=1,\n",
    "                                   bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n",
    "                                   return_dict_in_generate=True,)\n",
    "    \n",
    "        predictions = []\n",
    "        for seq in self.processor.tokenizer.batch_decode(outputs.sequences):\n",
    "            seq = seq.replace(self.processor.tokenizer.eos_token, \"\").replace(self.processor.tokenizer.pad_token, \"\")\n",
    "            seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
    "            predictions.append(seq)\n",
    "\n",
    "        scores = []\n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            pred = re.sub(r\"(?:(?<=>) | (?=</s_))\", \"\", pred)\n",
    "            # NOT NEEDED ANYMORE\n",
    "            # answer = re.sub(r\"<.*?>\", \"\", answer, count=1)\n",
    "            answer = answer.replace(self.processor.tokenizer.eos_token, \"\")\n",
    "            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n",
    "\n",
    "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
    "                print(f\"Prediction: {pred}\")\n",
    "                print(f\"    Answer: {answer}\")\n",
    "                print(f\" Normed ED: {scores[0]}\")\n",
    "\n",
    "        self.log(\"val_edit_distance\", np.mean(scores))\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # you could also add a learning rate scheduler if you want\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.get(\"lr\"))\n",
    "    \n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader\n",
    "config = {\"max_epochs\":12,\n",
    "          \"val_check_interval\":0.2, # how many times we want to validate during an epoch\n",
    "          \"check_val_every_n_epoch\":1,\n",
    "          \"gradient_clip_val\":1.0,\n",
    "          \"num_training_samples_per_epoch\": 40,\n",
    "          \"lr\":3e-5,\n",
    "          \"train_batch_sizes\": [1],\n",
    "          \"val_batch_sizes\": [1],\n",
    "          # \"seed\":2022,\n",
    "          \"num_nodes\": 1,\n",
    "          \"warmup_steps\": 12, # 800/8*30/10, 10%\n",
    "          \"result_path\": r\"E:\\result\",\n",
    "          \"verbose\": True,\n",
    "          }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "207dfd0a-820f-4a4d-a2b1-57cfa15620fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: logs\\Donut-demo-run-cord\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                      | Params\n",
      "----------------------------------------------------\n",
      "0 | model | VisionEncoderDecoderModel | 201 M \n",
      "----------------------------------------------------\n",
      "201 M     Trainable params\n",
      "0         Non-trainable params\n",
      "201 M     Total params\n",
      "403.821   Total estimated model params size (MB)\n",
      "E:\\anaconda\\envs\\donut\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "E:\\anaconda\\envs\\donut\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (23) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "E:\\anaconda\\envs\\donut\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d9a7ca6df54f18bd9970c7759a43fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\donut\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "model_module = DonutModelPLModule(config, processor, model)\n",
    "import os\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import Callback, EarlyStopping\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Replace WandbLogger with TensorBoardLogger\n",
    "tensorboard_logger = TensorBoardLogger(\"logs\", name=\"Donut-demo-run-cord\")\n",
    "\n",
    "class SaveFinalModelCallback(Callback):\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(\"Training done, saving the final model\")\n",
    "        # Save the final model to a local directory\n",
    "        local_model_dir = (r\"E:\\model\")\n",
    "        trainer.save_checkpoint(local_model_dir)\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_edit_distance\", patience=3, verbose=False, mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=config.get(\"max_epochs\"),\n",
    "    val_check_interval=config.get(\"val_check_interval\"),\n",
    "    check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
    "    gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
    "    precision=16,  # we'll use mixed precision\n",
    "    num_sanity_val_steps=0,\n",
    "    logger=tensorboard_logger,  # Use TensorBoardLogger instead of WandbLogger\n",
    "    callbacks=[SaveFinalModelCallback()],\n",
    ")\n",
    "\n",
    "trainer.fit(model_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f7fef3-aa50-466f-b0d1-9bf9bc731081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
